\documentclass[letterpaper, 10 pt, conference]{IEEEconf}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[style=ieee]{biblatex}
\usepackage{tikz}
\usetikzlibrary{positioning,fit,arrows.meta,backgrounds}
\usepackage{amsmath, amssymb, xcolor, tikz, pgfplots, pgfplotstable}

\newcommand{\todo}[1]{{\color{red}#1}}
\usepackage{listings, xcolor}

\colorlet{punct}{red!60!black}
\definecolor{background}{HTML}{EEEEEE}
\definecolor{delim}{RGB}{20,105,176}
\colorlet{numb}{magenta!60!black}

\lstdefinelanguage{json}{
    basicstyle=\normalfont\ttfamily,
    numbers=left,
    numberstyle=\scriptsize,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    frame=lines,
    backgroundcolor=\color{background},
    literate=
     *{0}{{{\color{numb}0}}}{1}
      {1}{{{\color{numb}1}}}{1}
      {2}{{{\color{numb}2}}}{1}
      {3}{{{\color{numb}3}}}{1}
      {4}{{{\color{numb}4}}}{1}
      {5}{{{\color{numb}5}}}{1}
      {6}{{{\color{numb}6}}}{1}
      {7}{{{\color{numb}7}}}{1}
      {8}{{{\color{numb}8}}}{1}
      {9}{{{\color{numb}9}}}{1}
      {:}{{{\color{punct}{:}}}}{1}
      {,}{{{\color{punct}{,}}}}{1}
      {\{}{{{\color{delim}{\{}}}}{1}
      {\}}{{{\color{delim}{\}}}}}{1}
      {[}{{{\color{delim}{[}}}}{1}
      {]}{{{\color{delim}{]}}}}{1},
}


\title{\LARGE \bf
A Pipeline to Assemble (Near-)Collision Footage Datasets
}

\author{
         Ben Upenieks, Chaitanya Varier,\\
         Curtis Duy Kha Phan, Jack David Roberts Williamson,\\
         Nicholas Geofroy, Tony Meng, Vincent-Olivier Roch\\
         University of Waterloo\\
         \\
         \tt\small ben.upenieks@uwaterloo.ca, cvarier@uwaterloo.ca,
         \\ \tt\small cdkphan@uwaterloo.ca, jdrwilli@uwaterloo.ca,
         \\ \tt\small nicholas.geofroy@uwaterloo.ca, c24meng@uwaterloo.ca, vroch@uwaterloo.ca
}

\bibliography{sources}
\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

The international movement towards developing autonomous vehicle technologies has raised concerns in all spheres of civilization: from legal perspectives to ethical dilemmas; from control theory to safety engineering; from cybersecurity to realtime computing. Over the
last decade, some emphasis has been made on the need for these vehicles to reduce road collisions and reckless driving. However, research into collision avoidance have been slowed down by the lack of egocentric footage dataset. Currently, these types of datasets are most effectively created through computer generated scenarios because of the dangerous and costly nature of road collisions. We provide a pipeline to assemble datasets of real events that have been published online to complement alredy existing methods. Our aggregated dataset includes egocentric vehicle footage from all over the world: varying visibilities, varying weather conditions, and varying settings.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

In the advent of connected vehicles and autonomous transportation, equipping consumer vehicles with the necessary equipment to facilitate high-end automated collision avoidance systems can be prohibitively expensive.
Technologies such as Radar, LIDAR and high-end cameras that are typically used for collision prediction are likely too expensive for most consumers.
On the other hand dash-cams, cheap video cameras that can be mounted to a car's dashboard, have become increasingly affordable and prevalent as a means to capture a driverâ€™s perspective.
Typically used for insurance purposes, dash-cams have the added benefit of recording a wide variety of collisions that wouldn't be feasible to simulate. 
This has led to the creation of many online communities to share videos of these rare situations on websites such as Reddit, Youtube, and Imgur.
We propose that this egocentric dash-cam video stream can be leveraged by computer vision systems to act as an affordable and commonplace sensor for automated collision prediction. 

In this paper we present a software pipeline to facilitate the collection, processing and annotation of egocentric dash-cam videos from various online video repositories to produce datasets that can be used to train deep dash-cam collision prediction models.
We aim to generate datasets that are not constrained to any regions or countries but rather provide location-agnostic data that is representative of general, global road and vehicle conditions.
These datasets will also be collision-oriented as the majority of these videos contain collisions for people's entertainment.
Current autonomous vehicle data collection techniques typically require astronomically expensive hardware and equipment and, consequently, are rarely involved in collisions and intentionally not put in high risk environments that may lead to a collision.
This is the data we hope to capture by taking advantage of existing videos of non-artificial collisions.

The pipeline uses web-scraping techniques to fetch the video data from these websites, along with a set of video metadata, describing the video's contents.
We use this data, in combination with the manually annotated data in order to create the collision dataset.
This combination allows us to take advantage of the wide variety of videos available on the internet, while still having reliable data that has been curated manually.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{RELATED WORK}

Chan et al. \cite{chan2016anticipating} approach the same problem of collision prediction via egocentric dash-cam videos. They propose a novel Dynamic-Spatial-Attention Recurrent Neural Network that distributes attention to localized and tracked vehicles within the video and models temporal dependencies to predict the earliest frame of a potential impending collision. Released with their paper is a dataset of 678 dash-cam videos capturing areas in Taiwan. Each video is annotated with vehicle tracking bounding boxes, the vehicles that were involved in the collision and the frame of the collision. We hope to allow researchers to expand on this dataset with location-agnostic data and similar annotations.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{METHODOLOGY}

The software pipeline is built of multiple executors that each process or handle the data corresponding to a single video before piping the resulting data to the next executor.


\begin{figure}[htpb]
		\centering
    \tikzset{
        module/.style={%
            draw, rounded corners,
            minimum width=#1,
            minimum height=7mm,
            font=\sffamily
            },
        module/.default=2cm,
        >=LaTeX
    }
    \begin{tikzpicture}[
      % This will show the frame around the figure
      show background rectangle]

      % Place first 6 items
      \node[module] (crawler) {Web-Scrapers};
      \node[module, below=of crawler] (downloader) {Downloaders};
      \node[module, below=of downloader] (splitter) {Video-Splitter};
      \node[module, below=of splitter] (labeller) {Video-Labelling Tool};
      \node[module, below=of labeller] (uploader) {Data-Uploader};
      \draw[->] (crawler)--(downloader);
      \draw[->] (downloader)--(splitter);
      \draw[->] (splitter)--(labeller);
      \draw[->] (labeller)--(uploader);

    \end{tikzpicture}
		\caption{Architecture of the default pipeline}
		\label{fig:obj-by-class}
\end{figure}


\subsection{Video Collection}

The entry into our pipeline is a set of web scraper executors that each target and scrape a specific website or video repository based on a user-defined set of key-terms, and an executor that pulls unlabelled videos from the user's database. Videos that are pulled by the scrapers are immediately added to the database with relevant metadata and marked as unlabelled. Subsequently, they are loaded into memory and sequentially fed into the pipeline. If the pipeline terminates before labelling a video, it remains flagged as unlabelled in the database and the video will be pulled in a subsequent run by the database source executor. Crawler state is maintained via database lookup of specified key-value pairs to ensure duplicate data is not pulled. No video preprocessing such as resizing, trimming or cropping is automatically performed.

Users may define their own search terms to be used when querying the video repository. This value can be set in the yaml pipeline configuration file. By default, our scrapers query using the keywords: \texttt{Car crash dashcam}. Currently we support scraping and querying Reddit, YouTube and Imgur. Users that want to pull from a video repository that is not currently implemented should create a custom executor and implement the \texttt{iCrawler} interface. This new executor should be added as an entry point in a new yaml configuration and should generate and return a \texttt{MetaDataItem} with video source information to be used by a subsequent and corresponding downloader. \texttt{MetaDataItem} objects are fed into the \texttt{UniversalDownloader} which aggregates \texttt{iDownloader} implementations and distributes \texttt{MetaDataItem} objects to their corresponding downloader via regex matching of source URL's. New downloaders should registered with the \texttt{UniversalDownloader} via the yaml pipeline configuration along with a regex string for url matching.  \todo{Maybe this should be refactored to just pass iCrawler output directly to its downloader}

% HAS BEEN REMOVED BECAUSE NOT FUNCTIONAL
%\subsection{Video Preprocessing}
%\subsection{Automatic Annotation}
%\subsection{Object Detection \& Tracking}
%\todo{Automated object detection not used right now?}
%\subsection{Anonymization}
%An important facet of automobile data aggregation is in eliminating biases and protecting the identities of depicted individuals. Consequently, our pipeline contains a post-processing stage in which we apply blurring masks to anonymize any personally identifiable information. Specifically, we provide two executors to perform these anonymization tasks: \texttt{FaceBlurrer}, which blurs faces and \texttt{LicensePlateBlurrer} which blurs license plates. Both of these executors are powered by pre-trained machine learning models which detect regions containing personally identifiable information. Specifically, for  \texttt{FaceBlurrer}, we use a MobileNet Single Shot MultiBox Detector (TODO: cite https://github.com/yeephycho/tensorflow-face-detection) and for \texttt{LicensePlateBlurrer} we use a Convolutional Neural Network (TODO: cite http://sergiomsilva.com/pubs/alpr-unconstrained/).
%
%This software module is encapsulated as a library and can be reused by importing our \texttt{anonymization} package as a dependency. The accuracy, amount and type of blurring applied can be tweaked using the API exposed by the \texttt{anonymization} package.

\subsection{Annotation}

In addition to collection and preprocessing, a core component of our pipeline is a GUI tool that facilitates the manual annotation of the dash-cam videos, and quick manual filtering of non-dashcam and irrelevant videos. Following Chan et al.\cite{chan2016anticipating}, if a collision occurs we provide tools to annotate the frame the collision occurs, and bounding boxes to localize, identify and track agents both involved and not involved in the collision. Object tracking bounding boxes can be linearly interpolated from manual start and end points to improve ease of labelling. \todo{We don't do this??? We also save the video resolution at which the video was annotated to ensure bounding box annotations can always be scaled to fit any variable download resolutions}. For long-form videos that may contain multiple collision events or irrelevant video segments, such as compilation videos or videos with substantial editing and transitions, we provide a video clipping feature to demarcate the collision events from which we extract and generate distinct data points. 
Each video segment will eventually become a metadata item. 

Once clipping is complete, each video must be manually labelled. Each object within a video segment must be bounded at every frame. To fasten this process, we provide an interpolation feature which will fill gaps between two entered bounding boxes. 

\subsection{Metadata}
After all of the previous steps have been completed, the pipeline is able to produce a metadata object.
There are many fields in the metadata object (as can be seen in Figure \ref{metadata_example}):

\begin{itemize}
  \item "collision\_type" contains the type of collision in the video (e.g. car vs. car)
  \item "description" contains a breif description of the video as scraped from the video hosting website
  \item "download\_src" contains the site that the video was sourced from (e.g. YouTube)
  \item "tags" is an object that contains custom user-defined tags on videos and additional data that is site specific
  \item "title" is a string that represents the type of the video
  \item "url" is the url that the video was fetched from
  \item "is\_split\_url" is a boolean that represents if all of the frames of the video have bounding boxes in this metadata
  \item "start\_i" is the start frame of the video that the bounding boxes are created for (if is\_split\_url is true)
  \item "end\_i" is the end frame of the video the bounding boxes are created for (if is\_split\_url is true)
  \item "bb\_fields" contains all bounding-box specific data. Each entry is a list, where the $i_{th}$ index in each contains the data for one bounding-box:
  \begin{itemize}
    \item "ids[$i$]" represents what vehicle id the $i_{th}$ bounding box represents. Each vehicle in the video has it's own unique id
    \item "cls[$i$]" represents the object class of the vehicle
    \item "frames[$i$]" represents what frame of the video the $i_{th}$ bounding box is for
    \item "has\_collision[$i$]" represents if the vehicle with id $ids[i]$ is in a collision
    \item "x1s[$i$], y1s[$i$], x2s[$i$], y2s[$i$]" are the coordinates of the two points $(x_1,y_1)$ \& $(x_2,y_2)$ that make bounding box $i$
  \end{itemize}
\end{itemize}`

\begin{figure}
  \begin{lstlisting}[language=json]
    {
      "collision_type":"",
      "description":"",
      "download_src":"",
      "tags": {
        "reddit_post_info":{
          "id":"<post_id>"
          "title":"title"
        }
      },
      "title":"Title of the Video",
      "url":"http:/www.myvideo.com",
      "is_split_url": false,
      "start_i": 0,
      "end_i": 60,
      "bb_fields": {
        "cls": [],
        "frames": [],
        "has_collision": [],
        "ids": [],
        "x1s": [],
        "y1s": [],
        "x2s": [],
        "y2s": [],
      }
    }
  \end{lstlisting}
    \caption{Example Metadata Produced by the Pipeline}
    \label{metadata_example}
\end{figure}

\begin{figure}[htpb]
		\centering
		% TODO: Uncomment this
    %\includegraphics[width=0.5\textwidth]{images/example_gui_tool.png}
		\caption{GUI tool to facilitate annotation}
		\label{fig:example_gui_tool-png}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{EXPERIMENT}

\todo{We are currently still assembling our dataset. At the time of this writing, we only have 67 positive videos labelled. By using many heuristics like splitting video segments into 5s snippets, we can duplicate our dataset
by introducing negatives from positive labelled videos. However, we must be concious to preserve a near 1:1 positives to negatives ratio in our testing set. Because of our low turnout, some of the results described below are likely inaccurate and subject to change.
}

\pgfplotstableread[col sep=comma]{by-classes.csv}\byclass
\pgfplotstableread[col sep=comma]{by-actors.csv}\byactors
\pgfplotstableread[col sep=comma]{by-agents.csv}\byagents
\pgfplotstableread[col sep=comma]{by-duration.csv}\byduration

We have aggregated some statistics about our dataset to help clients understand various distributions in the data. While these distributions may be representative of online collision footage, they are not necessarily of real world collisions. 

\begin{figure}[htpb]
		\centering
    \begin{tikzpicture}
    \begin{axis}[
      title={Objects by class},
      ylabel={Frequency (\# of objects)},
      xtick=data,
      xticklabels from table={\byclass}{X},
      xticklabel style={rotate=90}
    ]
    \addplot[fill, ybar] table [x expr=\coordindex, y={Y}] \byclass;
    \end{axis}
    \end{tikzpicture}
		\caption{Object by classes in our private database}
		\label{fig:obj-by-class}
\end{figure}

\begin{figure}[htpb]
		\centering
    \begin{tikzpicture}
    \begin{axis}[
      title={Videos by actors},
      xlabel={Number of actors},
      ylabel={Frequency (\# of videos)},
      xtick=data,
      xticklabels from table={\byactors}{X}
    ]
    \addplot[fill, ybar] table [x expr=\coordindex, y={Y}] \byactors;
    \end{axis}
    \end{tikzpicture}
		\caption{Videos by number of actors}
		\label{fig:vids-by-actors}
\end{figure}

\begin{figure}[htpb]
		\centering
    \begin{tikzpicture}
    \begin{axis}[
      title={Videos by agents},
      xlabel={Number of agents},
      ylabel={Frequency (\# of videos)},
      xtick=data,
      xticklabels from table={\byagents}{X}
    ]
    \addplot[fill, ybar] table [x expr=\coordindex, y={Y}] \byagents;
    \end{axis}
    \end{tikzpicture}
		\caption{Videos by number of agents}
		\label{fig:vids-by-agents}
\end{figure}

\begin{figure}[htpb]
		\centering
    \begin{tikzpicture}
    \begin{axis}[
      title={Videos by duration},
      xlabel={Duration (in frames)},
      ylabel={Frequency (\# of videos)},
      xtick=data,
      xticklabels from table={\byduration}{X}
    ]
    \addplot[fill, ybar] table [x expr=\coordindex, y={Y}] \byduration;
    \end{axis}
    \end{tikzpicture}
		\caption{Videos by duration}
		\label{fig:vids-by-duration}
\end{figure}

\begin{figure}[htpb]
		\centering
    \includegraphics[width=3in]{by-collision.png}
		\caption{Videos by collision}
		\label{fig:vids-by-collision}
\end{figure}

\begin{figure}[htpb]
		\centering
    \includegraphics[width=3in]{by-camera.png}
		\caption{Videos by camera}
		\label{fig:vids-by-camera}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{CONCLUSION}

%\todo{Write the conclusion}
%A conclusion section is not required. Although a conclusion may review the main points of the paper, do not replicate the abstract as the conclusion. A conclusion might elaborate on the importance of the work or suggest applications and extensions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{FUTURE WORK}

There are many ways that we could improve on our system. This are just a series of ideas that we could think of:
\begin{enumerate}
\item Custom interpolation between input frames: One idea would be to make it easier to modify the method of interpolation between frames. Currently, linear interpolation is always used. Moreover, given a type of interpolation is stored in the metadata, we probably shouldn't store all interpolated frames, rather than just the ones the user has entered. While linear interpolation works great for entering a frame every 10 frames, supporting more interpolations could be great for faster data entry, or even for supporting affine transformations on more complex bounds.

\item Polyhedral bounds: It's always possible to represent a 2-dimensional polyhedron with a matrix $A\in\mathbb{R}^{n\times 2}$, and a vector $b\in\mathbb{R}^2$, given that the polyhedron can be represented by the set $\left\{x\mid Ax\leq b\right\}$. We could use this idea to store masking bounds that can be used for training Mask-RCNN networks in (near-)collision contexts. This however would be 

\item Web interface: Having a web-based user interface would be advantageous for this project. While currently being used in the context of (near-)collision accident, this software has the potential to become a platform for scraping automation and collaboration. Ideally, people without a technical background would be able to contribute to the microtasking involved, and technical contributors would be in charge of creating and managing the automation and general design of the pipeline.

\end{enumerate}

\addtolength{\textheight}{-12cm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\nocite{*}
\printbibliography

\end{document}
